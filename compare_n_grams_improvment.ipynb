{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:53.737607Z",
     "start_time": "2025-05-07T20:23:53.734425Z"
    }
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from rouge_score import rouge_scorer\n",
    "import Levenshtein\n",
    "import matplotlib as plt"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:53.760076Z",
     "start_time": "2025-05-07T20:23:53.753001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load data for evaluation (loki generated atomic claims)\n",
    "with open('translate/atomic_claims_FCGPT_claude_translated.json', 'r') as file:\n",
    "    loki_translated = json.load(file)\n",
    "\n",
    "with open('translate/translated.json', 'r') as file:\n",
    "    dataset_translated = json.load(file)\n"
   ],
   "id": "7f01c33519ff89b1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:53.766505Z",
     "start_time": "2025-05-07T20:23:53.763713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process dataset to organize claims by comment\n",
    "gt_comments = []\n",
    "for entry in dataset_translated:\n",
    "    claims = []\n",
    "    for item in entry.get('claims', []):\n",
    "        if isinstance(item, list):\n",
    "            claims.extend(item)\n",
    "        elif isinstance(item, dict):\n",
    "            claims.append(item.get('claim', ''))\n",
    "        elif isinstance(item, str):\n",
    "            claims.append(item)\n",
    "    gt_comments.append(claims)\n",
    "\n",
    "loki_comments = []\n",
    "for entry in loki_translated:\n",
    "    claims = []\n",
    "    for item in entry.get('claims', []):\n",
    "        if isinstance(item, list):\n",
    "            claims.extend(item)\n",
    "        elif isinstance(item, dict):\n",
    "            claims.append(item.get('claim', ''))\n",
    "        elif isinstance(item, str):\n",
    "            claims.append(item)\n",
    "    loki_comments.append(claims)"
   ],
   "id": "6ec4a0370ceb32c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:53.775727Z",
     "start_time": "2025-05-07T20:23:53.773867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "def compute_rouge_f1(reference, candidate):\n",
    "    scores = scorer.score(reference, candidate)\n",
    "\n",
    "    # Here we use the F1 score for each ROUGE metric; you can adjust this if needed\n",
    "    rouge1_f1 = scores['rouge1'].fmeasure\n",
    "    rouge2_f1 = scores['rouge2'].fmeasure\n",
    "    rougeL_f1 = scores['rougeL'].fmeasure\n",
    "\n",
    "    # for simplicity, you might average these scores\n",
    "    avg_f1 = np.mean([rouge1_f1, rouge2_f1, rougeL_f1])\n",
    "    return avg_f1"
   ],
   "id": "f0b12fde2ee6365f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:54.575662Z",
     "start_time": "2025-05-07T20:23:53.789883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_scores = []\n",
    "all_pairs = []\n",
    "comment_metrics = []\n",
    "threshold = 0.7\n",
    "\n",
    "for i, (gt_claims_comment, gen_claims_comment) in enumerate(zip(gt_comments, loki_comments)):\n",
    "    # Skip empty comments\n",
    "    if not gt_claims_comment or not gen_claims_comment:\n",
    "        continue\n",
    "\n",
    "    # Create similarity matrix for this comment\n",
    "    num_gt = len(gt_claims_comment)\n",
    "    num_gen = len(gen_claims_comment)\n",
    "\n",
    "    # Handle unequal number of claims\n",
    "    max_claims = max(num_gt, num_gen)\n",
    "    similarity_matrix = np.zeros((max_claims, max_claims))\n",
    "\n",
    "    # Fill similarity matrix (missing claims get 0 similarity)\n",
    "    for i in range(min(num_gt, max_claims)):\n",
    "        for j in range(min(num_gen, max_claims)):\n",
    "            similarity_matrix[i, j] = compute_rouge_f1(gt_claims_comment[i], gen_claims_comment[j])\n",
    "\n",
    "    # Apply Hungarian algorithm\n",
    "    cost_matrix = -similarity_matrix\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # Get valid matches (ignoring padding)\n",
    "    valid_matches = [(r, c) for r, c in zip(row_ind, col_ind)\n",
    "                     if r < num_gt and c < num_gen]\n",
    "\n",
    "    if not valid_matches:\n",
    "        continue\n",
    "\n",
    "    # Calculate metrics for this comment\n",
    "    comment_scores = [similarity_matrix[r, c] for r, c in valid_matches]\n",
    "    avg_score = np.mean(comment_scores)\n",
    "\n",
    "    # Track matched pairs\n",
    "    comment_pairs = [\n",
    "        {\n",
    "            'Comment_Index': i,\n",
    "            'GT_Claim': gt_claims_comment[r],\n",
    "            'Generated_Claim': gen_claims_comment[c],\n",
    "            'Score': similarity_matrix[r, c]\n",
    "        }\n",
    "        for r, c in valid_matches\n",
    "    ]\n",
    "\n",
    "    # Calculate comment-level metrics\n",
    "    good_matches = sum(1 for score in comment_scores if score >= threshold)\n",
    "    recall = good_matches / num_gt if num_gt > 0 else 0\n",
    "    precision = good_matches / num_gen if num_gen > 0 else 0\n",
    "\n",
    "    # Add comment metrics\n",
    "    comment_metrics.append({\n",
    "        'Comment_Index': i,\n",
    "        'Avg_Score': avg_score,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'GT_Claims': num_gt,\n",
    "        'Gen_Claims': num_gen,\n",
    "        'Good_Matches': good_matches\n",
    "    })\n",
    "\n",
    "    # Add to overall scores and pairs\n",
    "    all_scores.extend(comment_scores)\n",
    "    all_pairs.extend(comment_pairs)"
   ],
   "id": "f910d84a95c4e530",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:23:54.584790Z",
     "start_time": "2025-05-07T20:23:54.582208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df_metrics = pd.DataFrame(comment_metrics)\n",
    "\n",
    "# Overall metrics\n",
    "overall_avg_score = np.mean(all_scores)\n",
    "overall_recall = np.mean(df_metrics['Recall'])\n",
    "overall_precision = np.mean(df_metrics['Precision'])\n",
    "\n",
    "print(f\"Overall Average Score: {overall_avg_score:.3f}\")\n",
    "print(f\"Overall Recall: {overall_recall:.2%}\")\n",
    "print(f\"Overall Precision: {overall_precision:.2%}\")"
   ],
   "id": "eee989ba8d57b61e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Average Score: 0.527\n",
      "Overall Recall: 22.73%\n",
      "Overall Precision: 20.11%\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
